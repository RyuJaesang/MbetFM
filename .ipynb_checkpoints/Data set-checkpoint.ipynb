{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-550043913c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"freq\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rule\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "A Python implementation of the FP-growth algorithm.\n",
    "Basic usage of the module is very simple:\n",
    "    > from fp_growth import find_frequent_itemsets\n",
    "    > find_frequent_itemsets(transactions, minimum_support)\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "__author__ = \"Eric Naeseth <eric@naeseth.com>\"\n",
    "__copyright__ = \"Copyright © 2009 Eric Naeseth\"\n",
    "__license__ = \"MIT License\"\n",
    "\n",
    "\n",
    "def find_frequent_itemsets(transactions, minimum_support, include_support=False):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets in the given transactions using FP-growth. This\n",
    "    function returns a generator instead of an eagerly-populated list of items.\n",
    "    The `transactions` parameter can be any iterable of iterables of items.\n",
    "    `minimum_support` should be an integer specifying the minimum number of\n",
    "    occurrences of an itemset for it to be accepted.\n",
    "    Each item must be hashable (i.e., it must be valid as a member of a\n",
    "    dictionary or a set).\n",
    "    If `include_support` is true, yield (itemset, support) pairs instead of\n",
    "    just the itemsets.\n",
    "    \"\"\"\n",
    "    items = defaultdict(lambda: 0)  # mapping from items to their supports\n",
    "\n",
    "    # if using support rate instead of support count\n",
    "    if 0 < minimum_support <= 1:\n",
    "        minimum_support = minimum_support * len(transactions)\n",
    "\n",
    "    # Load the passed-in transactions and count the support that individual\n",
    "    # items have.\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            items[item] += 1\n",
    "\n",
    "    # Remove infrequent items from the item support dictionary.\n",
    "    items = dict(\n",
    "        (item, support) for item, support in items.items() if support >= minimum_support\n",
    "    )\n",
    "\n",
    "    # Build our FP-tree. Before any transactions can be added to the tree, they\n",
    "    # must be stripped of infrequent items and their surviving items must be\n",
    "    # sorted in decreasing order of frequency.\n",
    "    def clean_transaction(transaction):\n",
    "        transaction = filter(lambda v: v in items, transaction)\n",
    "        transaction = sorted(transaction, key=lambda v: items[v], reverse=True)\n",
    "        return transaction\n",
    "\n",
    "    master = FPTree()\n",
    "    for transaction in list(map(clean_transaction, transactions)):\n",
    "        master.add(transaction)\n",
    "\n",
    "    def find_with_suffix(tree, suffix):\n",
    "        for item, nodes in tree.items():\n",
    "            support = sum(n.count for n in nodes)\n",
    "            if support >= minimum_support and item not in suffix:\n",
    "                # New winner!\n",
    "                found_set = [item] + suffix\n",
    "                yield (found_set, support) if include_support else found_set\n",
    "\n",
    "                # Build a conditional tree and recursively search for frequent\n",
    "                # itemsets within it.\n",
    "                cond_tree = conditional_tree_from_paths(tree.prefix_paths(item))\n",
    "                for s in find_with_suffix(cond_tree, found_set):\n",
    "                    yield s  # pass along the good news to our caller\n",
    "\n",
    "    # Search for frequent itemsets, and yield the results we find.\n",
    "    for itemset in find_with_suffix(master, []):\n",
    "        yield itemset\n",
    "\n",
    "\n",
    "class FPTree(object):\n",
    "    \"\"\"\n",
    "    An FP tree.\n",
    "    This object may only store transaction items that are hashable\n",
    "    (i.e., all items must be valid as dictionary keys or set members).\n",
    "    \"\"\"\n",
    "\n",
    "    Route = namedtuple(\"Route\", \"head tail\")\n",
    "\n",
    "    def __init__(self):\n",
    "        # The root node of the tree.\n",
    "        self._root = FPNode(self, None, None)\n",
    "\n",
    "        # A dictionary mapping items to the head and tail of a path of\n",
    "        # \"neighbors\" that will hit every node containing that item.\n",
    "        self._routes = {}\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"The root node of the tree.\"\"\"\n",
    "        return self._root\n",
    "\n",
    "    def add(self, transaction):\n",
    "        \"\"\"Add a transaction to the tree.\"\"\"\n",
    "        point = self._root\n",
    "\n",
    "        for item in transaction:\n",
    "            next_point = point.search(item)\n",
    "            if next_point:\n",
    "                # There is already a node in this tree for the current\n",
    "                # transaction item; reuse it.\n",
    "                next_point.increment()\n",
    "            else:\n",
    "                # Create a new point and add it as a child of the point we're\n",
    "                # currently looking at.\n",
    "                next_point = FPNode(self, item)\n",
    "                point.add(next_point)\n",
    "\n",
    "                # Update the route of nodes that contain this item to include\n",
    "                # our new node.\n",
    "                self._update_route(next_point)\n",
    "\n",
    "            point = next_point\n",
    "\n",
    "    def _update_route(self, point):\n",
    "        \"\"\"Add the given node to the route through all nodes for its item.\"\"\"\n",
    "        assert self is point.tree\n",
    "\n",
    "        try:\n",
    "            route = self._routes[point.item]\n",
    "            route[1].neighbor = point  # route[1] is the tail\n",
    "            self._routes[point.item] = self.Route(route[0], point)\n",
    "        except KeyError:\n",
    "            # First node for this item; start a new route.\n",
    "            self._routes[point.item] = self.Route(point, point)\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"\n",
    "        Generate one 2-tuples for each item represented in the tree. The first\n",
    "        element of the tuple is the item itself, and the second element is a\n",
    "        generator that will yield the nodes in the tree that belong to the item.\n",
    "        \"\"\"\n",
    "        for item in self._routes.keys():\n",
    "            yield (item, self.nodes(item))\n",
    "\n",
    "    def nodes(self, item):\n",
    "        \"\"\"\n",
    "        Generate the sequence of nodes that contain the given item.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            node = self._routes[item][0]\n",
    "        except KeyError:\n",
    "            return\n",
    "\n",
    "        while node:\n",
    "            yield node\n",
    "            node = node.neighbor\n",
    "\n",
    "    def prefix_paths(self, item):\n",
    "        \"\"\"Generate the prefix paths that end with the given item.\"\"\"\n",
    "\n",
    "        def collect_path(node):\n",
    "            path = []\n",
    "            while node and not node.root:\n",
    "                path.append(node)\n",
    "                node = node.parent\n",
    "            path.reverse()\n",
    "            return path\n",
    "\n",
    "        return (collect_path(node) for node in self.nodes(item))\n",
    "\n",
    "    def inspect(self):\n",
    "        print(\"Tree:\")\n",
    "        self.root.inspect(1)\n",
    "\n",
    "        print()\n",
    "        print(\"Routes:\")\n",
    "        for item, nodes in self.items():\n",
    "            print(\"  %r\" % item)\n",
    "            for node in nodes:\n",
    "                print(\"    %r\" % node)\n",
    "\n",
    "\n",
    "def conditional_tree_from_paths(paths):\n",
    "    \"\"\"Build a conditional FP-tree from the given prefix paths.\"\"\"\n",
    "    tree = FPTree()\n",
    "    condition_item = None\n",
    "    items = set()\n",
    "\n",
    "    # Import the nodes in the paths into the new tree. Only the counts of the\n",
    "    # leaf notes matter; the remaining counts will be reconstructed from the\n",
    "    # leaf counts.\n",
    "    for path in paths:\n",
    "        if condition_item is None:\n",
    "            condition_item = path[-1].item\n",
    "\n",
    "        point = tree.root\n",
    "        for node in path:\n",
    "            next_point = point.search(node.item)\n",
    "            if not next_point:\n",
    "                # Add a new node to the tree.\n",
    "                items.add(node.item)\n",
    "                count = node.count if node.item == condition_item else 0\n",
    "                next_point = FPNode(tree, node.item, count)\n",
    "                point.add(next_point)\n",
    "                tree._update_route(next_point)\n",
    "            point = next_point\n",
    "\n",
    "    assert condition_item is not None\n",
    "\n",
    "    # Calculate the counts of the non-leaf nodes.\n",
    "    for path in tree.prefix_paths(condition_item):\n",
    "        count = path[-1].count\n",
    "        for node in reversed(path[:-1]):\n",
    "            node._count += count\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "class FPNode(object):\n",
    "    \"\"\"A node in an FP tree.\"\"\"\n",
    "\n",
    "    def __init__(self, tree, item, count=1):\n",
    "        self._tree = tree\n",
    "        self._item = item\n",
    "        self._count = count\n",
    "        self._parent = None\n",
    "        self._children = {}\n",
    "        self._neighbor = None\n",
    "\n",
    "    def add(self, child):\n",
    "        \"\"\"Add the given FPNode `child` as a child of this node.\"\"\"\n",
    "\n",
    "        if not isinstance(child, FPNode):\n",
    "            raise TypeError(\"Can only add other FPNodes as children\")\n",
    "\n",
    "        if child.item not in self._children:\n",
    "            self._children[child.item] = child\n",
    "            child.parent = self\n",
    "\n",
    "    def search(self, item):\n",
    "        \"\"\"\n",
    "        Check whether this node contains a child node for the given item.\n",
    "        If so, that node is returned; otherwise, `None` is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._children[item]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self._children\n",
    "\n",
    "    @property\n",
    "    def tree(self):\n",
    "        \"\"\"The tree in which this node appears.\"\"\"\n",
    "        return self._tree\n",
    "\n",
    "    @property\n",
    "    def item(self):\n",
    "        \"\"\"The item contained in this node.\"\"\"\n",
    "        return self._item\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        \"\"\"The count associated with this node's item.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def increment(self):\n",
    "        \"\"\"Increment the count associated with this node's item.\"\"\"\n",
    "        if self._count is None:\n",
    "            raise ValueError(\"Root nodes have no associated count.\")\n",
    "        self._count += 1\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"True if this node is the root of a tree; false if otherwise.\"\"\"\n",
    "        return self._item is None and self._count is None\n",
    "\n",
    "    @property\n",
    "    def leaf(self):\n",
    "        \"\"\"True if this node is a leaf in the tree; false if otherwise.\"\"\"\n",
    "        return len(self._children) == 0\n",
    "\n",
    "    @property\n",
    "    def parent(self):\n",
    "        \"\"\"The node's parent\"\"\"\n",
    "        return self._parent\n",
    "\n",
    "    @parent.setter\n",
    "    def parent(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a parent.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a parent from another tree.\")\n",
    "        self._parent = value\n",
    "\n",
    "    @property\n",
    "    def neighbor(self):\n",
    "        \"\"\"\n",
    "        The node's neighbor; the one with the same value that is \"to the right\"\n",
    "        of it in the tree.\n",
    "        \"\"\"\n",
    "        return self._neighbor\n",
    "\n",
    "    @neighbor.setter\n",
    "    def neighbor(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a neighbor.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a neighbor from another tree.\")\n",
    "        self._neighbor = value\n",
    "\n",
    "    @property\n",
    "    def children(self):\n",
    "        \"\"\"The nodes that are children of this node.\"\"\"\n",
    "        return tuple(self._children.values())\n",
    "\n",
    "    def inspect(self, depth=0):\n",
    "        print((\"  \" * depth) + repr(self))\n",
    "        for child in self.children:\n",
    "            child.inspect(depth + 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.root:\n",
    "            return \"<%s (root)>\" % type(self).__name__\n",
    "        return \"<%s %r (%r)>\" % (type(self).__name__, self.item, self.count)\n",
    "\n",
    "\n",
    "def subs(l):\n",
    "    \"\"\"\n",
    "    Used for assoc_rule\n",
    "    \"\"\"\n",
    "    assert type(l) is list\n",
    "    if len(l) == 1:\n",
    "        return [l]\n",
    "    x = subs(l[1:])\n",
    "    return x + [[l[0]] + y for y in x]\n",
    "\n",
    "\n",
    "# Association rules\n",
    "def assoc_rule(freq, min_conf=0.6):\n",
    "    \"\"\"\n",
    "    This assoc_rule must input a dict for itemset -> support rate\n",
    "    And also can customize your minimum confidence\n",
    "    \"\"\"\n",
    "    assert type(freq) is dict\n",
    "    result = []\n",
    "    for item, sup in freq.items():\n",
    "        for subitem in subs(list(item)):\n",
    "            sb = [x for x in item if x not in subitem]\n",
    "            if sb == [] or subitem == []:\n",
    "                continue\n",
    "            if len(subitem) == 1 and (subitem[0][0] == \"in\" or subitem[0][0] == \"out\"):\n",
    "                continue\n",
    "            conf = sup / freq[tuple(subitem)]\n",
    "            if conf >= min_conf:\n",
    "                result.append({\"from\": subitem, \"to\": sb, \"sup\": sup, \"conf\": conf})\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from optparse import OptionParser\n",
    "    import csv\n",
    "\n",
    "    p = OptionParser(usage=\"%prog data_file\")\n",
    "    p.add_option(\n",
    "        \"-s\",\n",
    "        \"--minimum-support\",\n",
    "        dest=\"minsup\",\n",
    "        type=\"int\",\n",
    "        help=\"Minimum itemset support (default: 2)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-n\",\n",
    "        \"--numeric\",\n",
    "        dest=\"numeric\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Convert the values in datasets to numerals (default: false)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-c\",\n",
    "        \"--minimum-confidence\",\n",
    "        dest=\"minconf\",\n",
    "        type=\"float\",\n",
    "        help=\"Minimum rule confidence (default 0.6)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-f\",\n",
    "        \"--find\",\n",
    "        dest=\"find\",\n",
    "        type=\"str\",\n",
    "        help=\"Finding freq(frequency itemsets) or rule(association rules) (default: freq)\",\n",
    "    )\n",
    "    p.set_defaults(minsup=2)\n",
    "    p.set_defaults(numeric=False)\n",
    "    p.set_defaults(minconf=0.6)\n",
    "    p.set_defaults(find=\"freq\")\n",
    "    options, args = p.parse_args()\n",
    "\n",
    "    assert options.find == \"freq\" or options.find == \"rule\"\n",
    "\n",
    "    if len(args) < 1:\n",
    "        p.error(\"must provide the path to a CSV file to read\")\n",
    "\n",
    "    transactions = []\n",
    "    with open(args[0]) as database:\n",
    "        for row in csv.reader(database):\n",
    "            if options.numeric:\n",
    "                transaction = []\n",
    "                for item in row:\n",
    "                    transaction.append(int(item))\n",
    "                transactions.append(transaction)\n",
    "            else:\n",
    "                transactions.append(row)\n",
    "\n",
    "    result = []\n",
    "    res_for_rul = {}\n",
    "    for itemset, support in find_frequent_itemsets(transactions, options.minsup, True):\n",
    "        result.append((itemset, support))\n",
    "        res_for_rul[tuple(itemset)] = support\n",
    "\n",
    "    if options.find == \"freq\":\n",
    "        result = sorted(result, key=lambda i: i[0])\n",
    "        for itemset, support in result:\n",
    "            print(str(itemset) + \" \" + str(support))\n",
    "    if options.find == \"rule\":\n",
    "        rules = assoc_rule(res_for_rul, options.minconf)\n",
    "        for ru in rules:\n",
    "            print(str(ru[\"from\"]) + \" -> \" + str(ru[\"to\"]))\n",
    "            print(\"support = \" + str(ru[\"sup\"]) + \"confidence = \" + str(ru[\"conf\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-550043913c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"freq\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rule\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "A Python implementation of the FP-growth algorithm.\n",
    "Basic usage of the module is very simple:\n",
    "    > from fp_growth import find_frequent_itemsets\n",
    "    > find_frequent_itemsets(transactions, minimum_support)\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "__author__ = \"Eric Naeseth <eric@naeseth.com>\"\n",
    "__copyright__ = \"Copyright © 2009 Eric Naeseth\"\n",
    "__license__ = \"MIT License\"\n",
    "\n",
    "\n",
    "def find_frequent_itemsets(transactions, minimum_support, include_support=False):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets in the given transactions using FP-growth. This\n",
    "    function returns a generator instead of an eagerly-populated list of items.\n",
    "    The `transactions` parameter can be any iterable of iterables of items.\n",
    "    `minimum_support` should be an integer specifying the minimum number of\n",
    "    occurrences of an itemset for it to be accepted.\n",
    "    Each item must be hashable (i.e., it must be valid as a member of a\n",
    "    dictionary or a set).\n",
    "    If `include_support` is true, yield (itemset, support) pairs instead of\n",
    "    just the itemsets.\n",
    "    \"\"\"\n",
    "    items = defaultdict(lambda: 0)  # mapping from items to their supports\n",
    "\n",
    "    # if using support rate instead of support count\n",
    "    if 0 < minimum_support <= 1:\n",
    "        minimum_support = minimum_support * len(transactions)\n",
    "\n",
    "    # Load the passed-in transactions and count the support that individual\n",
    "    # items have.\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            items[item] += 1\n",
    "\n",
    "    # Remove infrequent items from the item support dictionary.\n",
    "    items = dict(\n",
    "        (item, support) for item, support in items.items() if support >= minimum_support\n",
    "    )\n",
    "\n",
    "    # Build our FP-tree. Before any transactions can be added to the tree, they\n",
    "    # must be stripped of infrequent items and their surviving items must be\n",
    "    # sorted in decreasing order of frequency.\n",
    "    def clean_transaction(transaction):\n",
    "        transaction = filter(lambda v: v in items, transaction)\n",
    "        transaction = sorted(transaction, key=lambda v: items[v], reverse=True)\n",
    "        return transaction\n",
    "\n",
    "    master = FPTree()\n",
    "    for transaction in list(map(clean_transaction, transactions)):\n",
    "        master.add(transaction)\n",
    "\n",
    "    def find_with_suffix(tree, suffix):\n",
    "        for item, nodes in tree.items():\n",
    "            support = sum(n.count for n in nodes)\n",
    "            if support >= minimum_support and item not in suffix:\n",
    "                # New winner!\n",
    "                found_set = [item] + suffix\n",
    "                yield (found_set, support) if include_support else found_set\n",
    "\n",
    "                # Build a conditional tree and recursively search for frequent\n",
    "                # itemsets within it.\n",
    "                cond_tree = conditional_tree_from_paths(tree.prefix_paths(item))\n",
    "                for s in find_with_suffix(cond_tree, found_set):\n",
    "                    yield s  # pass along the good news to our caller\n",
    "\n",
    "    # Search for frequent itemsets, and yield the results we find.\n",
    "    for itemset in find_with_suffix(master, []):\n",
    "        yield itemset\n",
    "\n",
    "\n",
    "class FPTree(object):\n",
    "    \"\"\"\n",
    "    An FP tree.\n",
    "    This object may only store transaction items that are hashable\n",
    "    (i.e., all items must be valid as dictionary keys or set members).\n",
    "    \"\"\"\n",
    "\n",
    "    Route = namedtuple(\"Route\", \"head tail\")\n",
    "\n",
    "    def __init__(self):\n",
    "        # The root node of the tree.\n",
    "        self._root = FPNode(self, None, None)\n",
    "\n",
    "        # A dictionary mapping items to the head and tail of a path of\n",
    "        # \"neighbors\" that will hit every node containing that item.\n",
    "        self._routes = {}\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"The root node of the tree.\"\"\"\n",
    "        return self._root\n",
    "\n",
    "    def add(self, transaction):\n",
    "        \"\"\"Add a transaction to the tree.\"\"\"\n",
    "        point = self._root\n",
    "\n",
    "        for item in transaction:\n",
    "            next_point = point.search(item)\n",
    "            if next_point:\n",
    "                # There is already a node in this tree for the current\n",
    "                # transaction item; reuse it.\n",
    "                next_point.increment()\n",
    "            else:\n",
    "                # Create a new point and add it as a child of the point we're\n",
    "                # currently looking at.\n",
    "                next_point = FPNode(self, item)\n",
    "                point.add(next_point)\n",
    "\n",
    "                # Update the route of nodes that contain this item to include\n",
    "                # our new node.\n",
    "                self._update_route(next_point)\n",
    "\n",
    "            point = next_point\n",
    "\n",
    "    def _update_route(self, point):\n",
    "        \"\"\"Add the given node to the route through all nodes for its item.\"\"\"\n",
    "        assert self is point.tree\n",
    "\n",
    "        try:\n",
    "            route = self._routes[point.item]\n",
    "            route[1].neighbor = point  # route[1] is the tail\n",
    "            self._routes[point.item] = self.Route(route[0], point)\n",
    "        except KeyError:\n",
    "            # First node for this item; start a new route.\n",
    "            self._routes[point.item] = self.Route(point, point)\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"\n",
    "        Generate one 2-tuples for each item represented in the tree. The first\n",
    "        element of the tuple is the item itself, and the second element is a\n",
    "        generator that will yield the nodes in the tree that belong to the item.\n",
    "        \"\"\"\n",
    "        for item in self._routes.keys():\n",
    "            yield (item, self.nodes(item))\n",
    "\n",
    "    def nodes(self, item):\n",
    "        \"\"\"\n",
    "        Generate the sequence of nodes that contain the given item.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            node = self._routes[item][0]\n",
    "        except KeyError:\n",
    "            return\n",
    "\n",
    "        while node:\n",
    "            yield node\n",
    "            node = node.neighbor\n",
    "\n",
    "    def prefix_paths(self, item):\n",
    "        \"\"\"Generate the prefix paths that end with the given item.\"\"\"\n",
    "\n",
    "        def collect_path(node):\n",
    "            path = []\n",
    "            while node and not node.root:\n",
    "                path.append(node)\n",
    "                node = node.parent\n",
    "            path.reverse()\n",
    "            return path\n",
    "\n",
    "        return (collect_path(node) for node in self.nodes(item))\n",
    "\n",
    "    def inspect(self):\n",
    "        print(\"Tree:\")\n",
    "        self.root.inspect(1)\n",
    "\n",
    "        print()\n",
    "        print(\"Routes:\")\n",
    "        for item, nodes in self.items():\n",
    "            print(\"  %r\" % item)\n",
    "            for node in nodes:\n",
    "                print(\"    %r\" % node)\n",
    "\n",
    "\n",
    "def conditional_tree_from_paths(paths):\n",
    "    \"\"\"Build a conditional FP-tree from the given prefix paths.\"\"\"\n",
    "    tree = FPTree()\n",
    "    condition_item = None\n",
    "    items = set()\n",
    "\n",
    "    # Import the nodes in the paths into the new tree. Only the counts of the\n",
    "    # leaf notes matter; the remaining counts will be reconstructed from the\n",
    "    # leaf counts.\n",
    "    for path in paths:\n",
    "        if condition_item is None:\n",
    "            condition_item = path[-1].item\n",
    "\n",
    "        point = tree.root\n",
    "        for node in path:\n",
    "            next_point = point.search(node.item)\n",
    "            if not next_point:\n",
    "                # Add a new node to the tree.\n",
    "                items.add(node.item)\n",
    "                count = node.count if node.item == condition_item else 0\n",
    "                next_point = FPNode(tree, node.item, count)\n",
    "                point.add(next_point)\n",
    "                tree._update_route(next_point)\n",
    "            point = next_point\n",
    "\n",
    "    assert condition_item is not None\n",
    "\n",
    "    # Calculate the counts of the non-leaf nodes.\n",
    "    for path in tree.prefix_paths(condition_item):\n",
    "        count = path[-1].count\n",
    "        for node in reversed(path[:-1]):\n",
    "            node._count += count\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "class FPNode(object):\n",
    "    \"\"\"A node in an FP tree.\"\"\"\n",
    "\n",
    "    def __init__(self, tree, item, count=1):\n",
    "        self._tree = tree\n",
    "        self._item = item\n",
    "        self._count = count\n",
    "        self._parent = None\n",
    "        self._children = {}\n",
    "        self._neighbor = None\n",
    "\n",
    "    def add(self, child):\n",
    "        \"\"\"Add the given FPNode `child` as a child of this node.\"\"\"\n",
    "\n",
    "        if not isinstance(child, FPNode):\n",
    "            raise TypeError(\"Can only add other FPNodes as children\")\n",
    "\n",
    "        if child.item not in self._children:\n",
    "            self._children[child.item] = child\n",
    "            child.parent = self\n",
    "\n",
    "    def search(self, item):\n",
    "        \"\"\"\n",
    "        Check whether this node contains a child node for the given item.\n",
    "        If so, that node is returned; otherwise, `None` is returned.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._children[item]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self._children\n",
    "\n",
    "    @property\n",
    "    def tree(self):\n",
    "        \"\"\"The tree in which this node appears.\"\"\"\n",
    "        return self._tree\n",
    "\n",
    "    @property\n",
    "    def item(self):\n",
    "        \"\"\"The item contained in this node.\"\"\"\n",
    "        return self._item\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        \"\"\"The count associated with this node's item.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "    def increment(self):\n",
    "        \"\"\"Increment the count associated with this node's item.\"\"\"\n",
    "        if self._count is None:\n",
    "            raise ValueError(\"Root nodes have no associated count.\")\n",
    "        self._count += 1\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        \"\"\"True if this node is the root of a tree; false if otherwise.\"\"\"\n",
    "        return self._item is None and self._count is None\n",
    "\n",
    "    @property\n",
    "    def leaf(self):\n",
    "        \"\"\"True if this node is a leaf in the tree; false if otherwise.\"\"\"\n",
    "        return len(self._children) == 0\n",
    "\n",
    "    @property\n",
    "    def parent(self):\n",
    "        \"\"\"The node's parent\"\"\"\n",
    "        return self._parent\n",
    "\n",
    "    @parent.setter\n",
    "    def parent(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a parent.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a parent from another tree.\")\n",
    "        self._parent = value\n",
    "\n",
    "    @property\n",
    "    def neighbor(self):\n",
    "        \"\"\"\n",
    "        The node's neighbor; the one with the same value that is \"to the right\"\n",
    "        of it in the tree.\n",
    "        \"\"\"\n",
    "        return self._neighbor\n",
    "\n",
    "    @neighbor.setter\n",
    "    def neighbor(self, value):\n",
    "        if value is not None and not isinstance(value, FPNode):\n",
    "            raise TypeError(\"A node must have an FPNode as a neighbor.\")\n",
    "        if value and value.tree is not self.tree:\n",
    "            raise ValueError(\"Cannot have a neighbor from another tree.\")\n",
    "        self._neighbor = value\n",
    "\n",
    "    @property\n",
    "    def children(self):\n",
    "        \"\"\"The nodes that are children of this node.\"\"\"\n",
    "        return tuple(self._children.values())\n",
    "\n",
    "    def inspect(self, depth=0):\n",
    "        print((\"  \" * depth) + repr(self))\n",
    "        for child in self.children:\n",
    "            child.inspect(depth + 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.root:\n",
    "            return \"<%s (root)>\" % type(self).__name__\n",
    "        return \"<%s %r (%r)>\" % (type(self).__name__, self.item, self.count)\n",
    "\n",
    "\n",
    "def subs(l):\n",
    "    \"\"\"\n",
    "    Used for assoc_rule\n",
    "    \"\"\"\n",
    "    assert type(l) is list\n",
    "    if len(l) == 1:\n",
    "        return [l]\n",
    "    x = subs(l[1:])\n",
    "    return x + [[l[0]] + y for y in x]\n",
    "\n",
    "\n",
    "# Association rules\n",
    "def assoc_rule(freq, min_conf=0.6):\n",
    "    \"\"\"\n",
    "    This assoc_rule must input a dict for itemset -> support rate\n",
    "    And also can customize your minimum confidence\n",
    "    \"\"\"\n",
    "    assert type(freq) is dict\n",
    "    result = []\n",
    "    for item, sup in freq.items():\n",
    "        for subitem in subs(list(item)):\n",
    "            sb = [x for x in item if x not in subitem]\n",
    "            if sb == [] or subitem == []:\n",
    "                continue\n",
    "            if len(subitem) == 1 and (subitem[0][0] == \"in\" or subitem[0][0] == \"out\"):\n",
    "                continue\n",
    "            conf = sup / freq[tuple(subitem)]\n",
    "            if conf >= min_conf:\n",
    "                result.append({\"from\": subitem, \"to\": sb, \"sup\": sup, \"conf\": conf})\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from optparse import OptionParser\n",
    "    import csv\n",
    "\n",
    "    p = OptionParser(usage=\"%prog data_file\")\n",
    "    p.add_option(\n",
    "        \"-s\",\n",
    "        \"--minimum-support\",\n",
    "        dest=\"minsup\",\n",
    "        type=\"int\",\n",
    "        help=\"Minimum itemset support (default: 2)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-n\",\n",
    "        \"--numeric\",\n",
    "        dest=\"numeric\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Convert the values in datasets to numerals (default: false)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-c\",\n",
    "        \"--minimum-confidence\",\n",
    "        dest=\"minconf\",\n",
    "        type=\"float\",\n",
    "        help=\"Minimum rule confidence (default 0.6)\",\n",
    "    )\n",
    "    p.add_option(\n",
    "        \"-f\",\n",
    "        \"--find\",\n",
    "        dest=\"find\",\n",
    "        type=\"str\",\n",
    "        help=\"Finding freq(frequency itemsets) or rule(association rules) (default: freq)\",\n",
    "    )\n",
    "    p.set_defaults(minsup=2)\n",
    "    p.set_defaults(numeric=False)\n",
    "    p.set_defaults(minconf=0.6)\n",
    "    p.set_defaults(find=\"freq\")\n",
    "    options, args = p.parse_args()\n",
    "\n",
    "    assert options.find == \"freq\" or options.find == \"rule\"\n",
    "\n",
    "    if len(args) < 1:\n",
    "        p.error(\"must provide the path to a CSV file to read\")\n",
    "\n",
    "    transactions = []\n",
    "    with open(args[0]) as database:\n",
    "        for row in csv.reader(database):\n",
    "            if options.numeric:\n",
    "                transaction = []\n",
    "                for item in row:\n",
    "                    transaction.append(int(item))\n",
    "                transactions.append(transaction)\n",
    "            else:\n",
    "                transactions.append(row)\n",
    "\n",
    "    result = []\n",
    "    res_for_rul = {}\n",
    "    for itemset, support in find_frequent_itemsets(transactions, options.minsup, True):\n",
    "        result.append((itemset, support))\n",
    "        res_for_rul[tuple(itemset)] = support\n",
    "\n",
    "    if options.find == \"freq\":\n",
    "        result = sorted(result, key=lambda i: i[0])\n",
    "        for itemset, support in result:\n",
    "            print(str(itemset) + \" \" + str(support))\n",
    "    if options.find == \"rule\":\n",
    "        rules = assoc_rule(res_for_rul, options.minconf)\n",
    "        for ru in rules:\n",
    "            print(str(ru[\"from\"]) + \" -> \" + str(ru[\"to\"]))\n",
    "            print(\"support = \" + str(ru[\"sup\"]) + \"confidence = \" + str(ru[\"conf\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>타임스탬프</th>\n",
       "      <th>What genre of music do you most likely to listen to? /가장 좋아하는 음악 장르는 무엇입니까?(Select 3 / 3가지 선택)</th>\n",
       "      <th>What kind of fashion style do you like the most? / 가장 좋아하는 패션 스타일은 무엇입니까? (Select 3 / 3가지 선택)</th>\n",
       "      <th>What kind of art performances do you like the most? / 가장 좋아하는 공연은 무엇입니까?</th>\n",
       "      <th>What kind of art exhibitions do you like the most? / 가장 좋아하는 전시회는 무엇입니까?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019. 11. 7 오후 6:03:37</td>\n",
       "      <td>Pop / Idol (팝/아이돌), Ballad (발라드), Dance (댄스)</td>\n",
       "      <td>Workwear (워크웨어륵) (constructed of sturdy fabric...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Sculptures (조각)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019. 11. 7 오후 6:04:10</td>\n",
       "      <td>Rap/ Hip Hop (랩/힙합), R&amp;B / Soul (R&amp;B/소울), Rock...</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Paintings (회화)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019. 11. 7 오후 6:10:23</td>\n",
       "      <td>Pop (팝), Ballad (발라드), R&amp;B / Soul (R&amp;B/소울)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Photography (사진)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019. 11. 7 오후 6:10:27</td>\n",
       "      <td>Pop (팝), Rap/ Hip Hop (랩/힙합), Indie (인디)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Art Design (디자인)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019. 11. 7 오후 6:10:44</td>\n",
       "      <td>Idol (아이돌), Ballad (발라드), Dance (댄스)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Musicals (뮤지컬)</td>\n",
       "      <td>Art Design (디자인)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    타임스탬프  \\\n",
       "0  2019. 11. 7 오후 6:03:37   \n",
       "1  2019. 11. 7 오후 6:04:10   \n",
       "2  2019. 11. 7 오후 6:10:23   \n",
       "3  2019. 11. 7 오후 6:10:27   \n",
       "4  2019. 11. 7 오후 6:10:44   \n",
       "\n",
       "  What genre of music do you most likely to listen to? /가장 좋아하는 음악 장르는 무엇입니까?(Select 3 / 3가지 선택)  \\\n",
       "0       Pop / Idol (팝/아이돌), Ballad (발라드), Dance (댄스)                                               \n",
       "1  Rap/ Hip Hop (랩/힙합), R&B / Soul (R&B/소울), Rock...                                               \n",
       "2         Pop (팝), Ballad (발라드), R&B / Soul (R&B/소울)                                               \n",
       "3           Pop (팝), Rap/ Hip Hop (랩/힙합), Indie (인디)                                               \n",
       "4               Idol (아이돌), Ballad (발라드), Dance (댄스)                                               \n",
       "\n",
       "  What kind of fashion style do you like the most? / 가장 좋아하는 패션 스타일은 무엇입니까? (Select 3 / 3가지 선택)  \\\n",
       "0  Workwear (워크웨어륵) (constructed of sturdy fabric...                                              \n",
       "1  Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "2  Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "3  Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "4  Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "\n",
       "  What kind of art performances do you like the most? / 가장 좋아하는 공연은 무엇입니까?  \\\n",
       "0                            Music Performances (음악)                         \n",
       "1                            Music Performances (음악)                         \n",
       "2                            Music Performances (음악)                         \n",
       "3                            Music Performances (음악)                         \n",
       "4                                     Musicals (뮤지컬)                         \n",
       "\n",
       "  What kind of art exhibitions do you like the most? / 가장 좋아하는 전시회는 무엇입니까?  \n",
       "0                                    Sculptures (조각)                        \n",
       "1                                     Paintings (회화)                        \n",
       "2                                   Photography (사진)                        \n",
       "3                                   Art Design (디자인)                        \n",
       "4                                   Art Design (디자인)                        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_data = pd.read_csv('bi_data.csv')\n",
    "list_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>타임스탬프</th>\n",
       "      <th>What genre of music do you most likely to listen to? /가장 좋아하는 음악 장르는 무엇입니까?(Select 3 / 3가지 선택)</th>\n",
       "      <th>What kind of fashion style do you like the most? / 가장 좋아하는 패션 스타일은 무엇입니까? (Select 3 / 3가지 선택)</th>\n",
       "      <th>What kind of art performances do you like the most? / 가장 좋아하는 공연은 무엇입니까?</th>\n",
       "      <th>What kind of art exhibitions do you like the most? / 가장 좋아하는 전시회는 무엇입니까?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019. 11. 7 오후 6:03:37</td>\n",
       "      <td>Pop / Idol (팝/아이돌), Ballad (발라드), Dance (댄스)</td>\n",
       "      <td>Workwear (워크웨어륵) (constructed of sturdy fabric...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Sculptures (조각)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019. 11. 7 오후 6:04:10</td>\n",
       "      <td>Rap/ Hip Hop (랩/힙합), R&amp;B / Soul (R&amp;B/소울), Rock...</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Paintings (회화)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019. 11. 7 오후 6:10:23</td>\n",
       "      <td>Pop (팝), Ballad (발라드), R&amp;B / Soul (R&amp;B/소울)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Photography (사진)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019. 11. 7 오후 6:10:27</td>\n",
       "      <td>Pop (팝), Rap/ Hip Hop (랩/힙합), Indie (인디)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Art Design (디자인)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019. 11. 7 오후 6:10:44</td>\n",
       "      <td>Idol (아이돌), Ballad (발라드), Dance (댄스)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Musicals (뮤지컬)</td>\n",
       "      <td>Art Design (디자인)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2019. 11. 15 오후 4:07:30</td>\n",
       "      <td>Pop (팝), Ballad (발라드), R&amp;B / Soul (R&amp;B/소울)</td>\n",
       "      <td>Casual (캐주얼룩), Street (스트릿룩), Grunge (그런지룩)</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Paintings (회화)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2019. 11. 15 오후 4:36:31</td>\n",
       "      <td>Pop (팝), R&amp;B / Soul (R&amp;B/소울), Indie (인디)</td>\n",
       "      <td>Street (스트릿룩), Vintage (빈티지룩), Chic (시크룩)</td>\n",
       "      <td>Musicals (뮤지컬)</td>\n",
       "      <td>Paintings (회화)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2019. 11. 15 오후 5:51:09</td>\n",
       "      <td>Pop (팝), Idol (아이돌), R&amp;B / Soul (R&amp;B/소울)</td>\n",
       "      <td>Workwear (워크웨어륵) (constructed of sturdy fabric...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Art Design (디자인)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>2019. 11. 16 오후 5:16:58</td>\n",
       "      <td>Pop (팝), Rap/ Hip Hop (랩/힙합), R&amp;B / Soul (R&amp;B/소울)</td>\n",
       "      <td>Casual (캐주얼룩), Street (스트릿룩), Grunge (그런지룩)</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Paintings (회화)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2019. 11. 19 오전 11:42:31</td>\n",
       "      <td>Idol (아이돌), Dance (댄스), Indie (인디)</td>\n",
       "      <td>Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...</td>\n",
       "      <td>Music Performances (음악)</td>\n",
       "      <td>Photography (사진)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        타임스탬프  \\\n",
       "0      2019. 11. 7 오후 6:03:37   \n",
       "1      2019. 11. 7 오후 6:04:10   \n",
       "2      2019. 11. 7 오후 6:10:23   \n",
       "3      2019. 11. 7 오후 6:10:27   \n",
       "4      2019. 11. 7 오후 6:10:44   \n",
       "..                        ...   \n",
       "161   2019. 11. 15 오후 4:07:30   \n",
       "162   2019. 11. 15 오후 4:36:31   \n",
       "163   2019. 11. 15 오후 5:51:09   \n",
       "164   2019. 11. 16 오후 5:16:58   \n",
       "165  2019. 11. 19 오전 11:42:31   \n",
       "\n",
       "    What genre of music do you most likely to listen to? /가장 좋아하는 음악 장르는 무엇입니까?(Select 3 / 3가지 선택)  \\\n",
       "0         Pop / Idol (팝/아이돌), Ballad (발라드), Dance (댄스)                                               \n",
       "1    Rap/ Hip Hop (랩/힙합), R&B / Soul (R&B/소울), Rock...                                               \n",
       "2           Pop (팝), Ballad (발라드), R&B / Soul (R&B/소울)                                               \n",
       "3             Pop (팝), Rap/ Hip Hop (랩/힙합), Indie (인디)                                               \n",
       "4                 Idol (아이돌), Ballad (발라드), Dance (댄스)                                               \n",
       "..                                                 ...                                               \n",
       "161         Pop (팝), Ballad (발라드), R&B / Soul (R&B/소울)                                               \n",
       "162           Pop (팝), R&B / Soul (R&B/소울), Indie (인디)                                               \n",
       "163           Pop (팝), Idol (아이돌), R&B / Soul (R&B/소울)                                               \n",
       "164  Pop (팝), Rap/ Hip Hop (랩/힙합), R&B / Soul (R&B/소울)                                               \n",
       "165                 Idol (아이돌), Dance (댄스), Indie (인디)                                               \n",
       "\n",
       "    What kind of fashion style do you like the most? / 가장 좋아하는 패션 스타일은 무엇입니까? (Select 3 / 3가지 선택)  \\\n",
       "0    Workwear (워크웨어륵) (constructed of sturdy fabric...                                              \n",
       "1    Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "2    Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "3    Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "4    Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "..                                                 ...                                              \n",
       "161        Casual (캐주얼룩), Street (스트릿룩), Grunge (그런지룩)                                              \n",
       "162          Street (스트릿룩), Vintage (빈티지룩), Chic (시크룩)                                              \n",
       "163  Workwear (워크웨어륵) (constructed of sturdy fabric...                                              \n",
       "164        Casual (캐주얼룩), Street (스트릿룩), Grunge (그런지룩)                                              \n",
       "165  Casual (캐주얼룩), Workwear (워크웨어륵) (constructed o...                                              \n",
       "\n",
       "    What kind of art performances do you like the most? / 가장 좋아하는 공연은 무엇입니까?  \\\n",
       "0                              Music Performances (음악)                         \n",
       "1                              Music Performances (음악)                         \n",
       "2                              Music Performances (음악)                         \n",
       "3                              Music Performances (음악)                         \n",
       "4                                       Musicals (뮤지컬)                         \n",
       "..                                                 ...                         \n",
       "161                            Music Performances (음악)                         \n",
       "162                                     Musicals (뮤지컬)                         \n",
       "163                            Music Performances (음악)                         \n",
       "164                            Music Performances (음악)                         \n",
       "165                            Music Performances (음악)                         \n",
       "\n",
       "    What kind of art exhibitions do you like the most? / 가장 좋아하는 전시회는 무엇입니까?  \n",
       "0                                      Sculptures (조각)                        \n",
       "1                                       Paintings (회화)                        \n",
       "2                                     Photography (사진)                        \n",
       "3                                     Art Design (디자인)                        \n",
       "4                                     Art Design (디자인)                        \n",
       "..                                                 ...                        \n",
       "161                                     Paintings (회화)                        \n",
       "162                                     Paintings (회화)                        \n",
       "163                                   Art Design (디자인)                        \n",
       "164                                     Paintings (회화)                        \n",
       "165                                   Photography (사진)                        \n",
       "\n",
       "[166 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_data = pd.read_csv('bi_data.csv')\n",
    "list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cd225bbf7136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-70a91cb26643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-70a91cb26643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-023f160cc61a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "data = sc.textFile(\"bi_data.csv\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ea4b2c3ce596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b078727c7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-144ab1860cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b078727c7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4e91d34768ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     34\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0a5f448048e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/spark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-39fb9ae5f680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b078727c7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFPGrowth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_data.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"bi_data.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
